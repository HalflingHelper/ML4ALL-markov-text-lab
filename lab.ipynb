{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Road to GPT\n",
    "### Generating text with simple python code.\n",
    "> This lab was inspired by this [blog post](https://benhoyt.com/writings/markov-chain/)\n",
    "\n",
    "Large Language Models (LLMs) such as ChatGPT, Claude, and Llama have become \n",
    "one of the most popular machine learning (ML) models. LLMs use vast amounts\n",
    "of data to learn the relationships between different words, and predict which\n",
    "word is statisticly more likely to occur next in a sequence. This notebook \n",
    "describes a simpler technique for statistical text generation using Markov \n",
    "chains and n-grams, important concepts for ML and Natural Language Processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Idea\n",
    "We want to choose the next word to use in a sequence of text.\n",
    "\n",
    "### Markov Chains\n",
    "Markov chains are a useful tool for this task. They\n",
    "describe a sequence of probabilistic events, where\n",
    "each event depends only on the current state.\n",
    "\n",
    "<!-- TODO: https://en.wikipedia.org/wiki/Markov_chain  -->\n",
    "Though not explicitly ML, have linear algebra,\n",
    "are a generally good thing to know about.\n",
    "\n",
    "### N-Grams\n",
    "Something we haven't talked about is how many words should be used to predict the next one.\n",
    "If this number is too low, we're way too random, and if it's too high\n",
    "\n",
    "<!-- TODO: https://en.wikipedia.org/wiki/N-gram -->\n",
    "These are used a lot in NLP, another good thing to know.\n",
    "\n",
    "\n",
    "### Temperature\n",
    "Another question is about how deterministic our model should be.\n",
    "That is: given some starting state, should we always generate the same thing?\n",
    "\n",
    "This concept is known as temperature. At a temperature of 0, the\n",
    "model will always choose the most likely option, and as it increases\n",
    "the probabilities become increasingly equally distributed.\n",
    "\n",
    "This is sometimes described as representing how \"creative\" a model\n",
    "will be.\n",
    "\n",
    "<!-- TODO: This equation isn't great: I -->\n",
    "$$P_i = \\frac{e^\\frac{y_i}{T}}{\\sum_{k=1}^ne^\\frac{y_k}{T}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bilbo could wait no longer. They would stand a siege for weeks, and by\n",
      "magic. Somebody kicked the sparks up in his hall, and you don’t know\n",
      "if it was heavy on him. He was lying a great distance. The Lord of the\n",
      "dragon’s jaws. He circled for a mercy and a\n"
     ]
    }
   ],
   "source": [
    "# TODO: This is the full code, we might want to make\n",
    "# a version of this with fill in the blanks for people\n",
    "# who might want to do it themselves\n",
    "import collections, random, sys, textwrap\n",
    "\n",
    "# Set some parameters\n",
    "f_name = \"hobbit.txt\" # Your input file\n",
    "output_size = 50     # How many words you want to generate\n",
    "n_grams = 2           # TODO: How many words do you want to use to predict?\n",
    "temperature = 1       # TODO: Could be a nice way to talk about this\n",
    "# TODO: Anything else? \n",
    "\n",
    "# Build possibles table indexed by pair of prefix words (w1, w2)\n",
    "w1 = w2 = ''\n",
    "possibles = collections.defaultdict(list)\n",
    "\n",
    "\n",
    "with open(f_name, \"r\", encoding=\"utf-8\") as words:\n",
    "    for line in words:\n",
    "        for word in line.split():\n",
    "            # TODO: Make this use the n_grams parameter\n",
    "            possibles[w1, w2].append(word)\n",
    "            w1, w2 = w2, word\n",
    "\n",
    "# Avoid empty possibles lists at end of input\n",
    "possibles[w1, w2].append('')\n",
    "possibles[w2, ''].append('')\n",
    "\n",
    "# Generate randomized output (start with a random capitalized prefix)\n",
    "w1, w2 = random.choice([k for k in possibles if k[0][:1].isupper()])\n",
    "output = [w1, w2]\n",
    "for _ in range(output_size):\n",
    "    # TODO: Add temperature.\n",
    "    word = random.choice(possibles[w1, w2])\n",
    "    output.append(word)\n",
    "    w1, w2 = w2, word\n",
    "\n",
    "# Print output wrapped to 70 columns\n",
    "print(textwrap.fill(' '.join(output)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So what does this have to do with ChatGPT?\n",
    "- Performs the same task\n",
    "- Much smaller sense of context\n",
    "- Doesn't understand word relations\n",
    "    - Talk about classic example of words as vectors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
