{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Road to GPT\n",
    "### Generating text with simple python code.\n",
    "> This lab was inspired by this [blog post](https://benhoyt.com/writings/markov-chain/)\n",
    "\n",
    "Large Language Models (LLMs) such as ChatGPT, Claude, and Llama have become \n",
    "one of the most popular machine learning (ML) models. LLMs use vast amounts\n",
    "of data to learn the relationships between different words, and predict which\n",
    "word is statisticly more likely to occur next in a sequence. This notebook \n",
    "describes a simpler technique for statistical text generation using Markov \n",
    "chains and n-grams, important concepts for ML and Natural Language Processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Idea\n",
    "We want to choose the next word to use in a sequence of text.\n",
    "\n",
    "### Markov Chains\n",
    "Markov chains are a useful tool for this task. They\n",
    "describe a sequence of probabilistic events, where\n",
    "each event depends only on the current state.\n",
    "\n",
    "<img src=\"images/Markov-Chains-in-NLP.webp\" width=\"400\">\n",
    "\n",
    "You can read more about the theory and applications of \n",
    "markov chains on [wikipedia](https://en.wikipedia.org/wiki/Markov_chain).\n",
    "\n",
    "For our use case, we care about the \"state\" as being the previous\n",
    "words in the sentence. For example, consider us predicting\n",
    "the next word in the following sentence:\n",
    "```\n",
    "For dinner, we will eat ___\n",
    "```\n",
    "Our state for predicting the last word of this sentence\n",
    "is \"eat\", and in choosing what comes next, we will only\n",
    "consider words that can come after eat.\n",
    "\n",
    "\n",
    "### N-Grams\n",
    "Clearly using one word to predict the next isn't ideal. For \n",
    "example, consider:\n",
    "```\n",
    "My favorite baseball team is the ___.\n",
    "```\n",
    "If our state is just \"the\", there are so many words that\n",
    "can come next. Is it `the pizza`, `the St. Louis Cardinals`, \n",
    "`the Indianapolis Colts`? Many of these options aren't ideal.\n",
    "\n",
    "Something that can mitigate this issue is using more than just one\n",
    "word to predict the next. If our state is \"baseball team is the\",\n",
    "then there are much fewer options for what can come next.\n",
    "\n",
    "These groupings of subsequent words are called *N-grams*,\n",
    "and are used commonly in NLP to capture information about word\n",
    "order. [wikipedia](https://en.wikipedia.org/wiki/N-gram)\n",
    "\n",
    "But be careful! If too many words are used to predict the next,\n",
    "then we run the risk of just reproducing the training input.\n",
    "\n",
    "\n",
    "### Temperature\n",
    "Another question is about how deterministic our model should be.\n",
    "That is: given some starting state, should we always generate the same thing?\n",
    "\n",
    "This concept is known as temperature. At a temperature of 0, the\n",
    "model will always choose the most likely option, and as it increases\n",
    "the probabilities become increasingly equally distributed.\n",
    "\n",
    "This is sometimes described as representing how \"creative\" a model\n",
    "will be.\n",
    "\n",
    "<!-- TODO: This equation isn't great: I -->\n",
    "$$P_i = \\frac{e^\\frac{y_i}{T}}{\\sum_{k=1}^ne^\\frac{y_k}{T}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I do now. I assure you, he is quite ready to sit on the doorstep\n",
      "lonesome and wearisome â€” there was not a doorstep, of course, really,\n",
      "but they used to call the little grassy bay. There they made their\n",
      "third camp, hauling up what they needed from below with their ropes.\n",
      "Down\n"
     ]
    }
   ],
   "source": [
    "# TODO: This is the full code, we might want to make\n",
    "# a version of this with fill in the blanks for people\n",
    "# who might want to do it themselves\n",
    "import collections, random, sys, textwrap\n",
    "\n",
    "# Set some parameters\n",
    "f_name = \"hobbit.txt\" # Your input file\n",
    "output_size = 50     # How many words you want to generate\n",
    "n_grams = 2           # TODO: How many words do you want to use to predict?\n",
    "temperature = 1       # TODO: Could be a nice way to talk about this\n",
    "# TODO: Anything else? \n",
    "\n",
    "# Build possibles table indexed by pair of prefix words (w1, w2)\n",
    "w1 = w2 = ''\n",
    "possibles = collections.defaultdict(list)\n",
    "\n",
    "\n",
    "with open(f_name, \"r\", encoding=\"utf-8\") as words:\n",
    "    for line in words:\n",
    "        for word in line.split():\n",
    "            # TODO: Make this use the n_grams parameter\n",
    "            possibles[w1, w2].append(word)\n",
    "            w1, w2 = w2, word\n",
    "\n",
    "# Avoid empty possibles lists at end of input\n",
    "possibles[w1, w2].append('')\n",
    "possibles[w2, ''].append('')\n",
    "\n",
    "# Generate randomized output (start with a random capitalized prefix)\n",
    "w1, w2 = random.choice([k for k in possibles if k[0][:1].isupper()])\n",
    "output = [w1, w2]\n",
    "for _ in range(output_size):\n",
    "    # TODO: Add temperature.\n",
    "    word = random.choice(possibles[w1, w2])\n",
    "    output.append(word)\n",
    "    w1, w2 = w2, word\n",
    "\n",
    "# Print output wrapped to 70 columns\n",
    "print(textwrap.fill(' '.join(output)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So what does this have to do with ChatGPT?\n",
    "- Performs the same task\n",
    "- Much smaller sense of context\n",
    "- Doesn't understand word relations\n",
    "    - Talk about classic example of words as vectors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
